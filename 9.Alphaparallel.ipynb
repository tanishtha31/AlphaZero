{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359010a9-cc92-4329-92db-36f19d5e9cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f0becb5-544a-4f12-9136-5cdc5748507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"TicTacToe\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "        \n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3cc2c0-e266-4a2d-be70-e6326a3522bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "        self.in_a_row = 4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"ConnectFour\"\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "    \n",
    "    def get_next_state(self, state, action, player):\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "    \n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "    \n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.in_a_row):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.column_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.in_a_row - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.in_a_row - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n",
    "        )\n",
    "    \n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "    \n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "    \n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "    \n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b2ed10-b393-45ec-b85c-e493eeb0ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.startBlock = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "        \n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "        \n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "        \n",
    "        \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7cbd2d-37a2-490f-8803-6c9b64371000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9436637163162231\n",
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "tensor([[[[0., 0., 1.],\n",
      "          [0., 1., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [1., 0., 1.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tictactoe = TicTacToe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, -1)\n",
    "state = tictactoe.get_next_state(state, 4, -1)\n",
    "state = tictactoe.get_next_state(state, 6, 1)\n",
    "state = tictactoe.get_next_state(state, 8, 1)\n",
    "\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64, device=device)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f45c44-6d3f-4d54-9ff2-299fdd8a86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.children = []\n",
    "        \n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "    \n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "        \n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "                \n",
    "        return best_child\n",
    "    \n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "    \n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "                \n",
    "        return child\n",
    "            \n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "        \n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)  \n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "        \n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n",
    "        \n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "            \n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "                policy /= np.sum(policy)\n",
    "                \n",
    "                value = value.item()\n",
    "                \n",
    "                node.expand(policy)\n",
    "                \n",
    "            node.backpropagate(value)    \n",
    "            \n",
    "            \n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "223b4c86-2709-46c7-94c4-a5858d9ddc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "        \n",
    "        while True:\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "            \n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "            \n",
    "            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "            action = np.random.choice(self.game.action_size, p=temperature_action_probs) # Divide temperature_action_probs with its sum in case of an error\n",
    "            \n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "            \n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "            \n",
    "            if is_terminal:\n",
    "                returnMemory = []\n",
    "                for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                    returnMemory.append((\n",
    "                        self.game.get_encoded_state(hist_neutral_state),\n",
    "                        hist_action_probs,\n",
    "                        hist_outcome\n",
    "                    ))\n",
    "                return returnMemory\n",
    "            \n",
    "            player = self.game.get_opponent(player)\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fae29f6-a2fb-4f16-92a0-c2f9ec6df250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def search(self, states, spGames):\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n",
    "            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size, size=policy.shape[0])\n",
    "        \n",
    "        for i, spg in enumerate(spGames):\n",
    "            spg_policy = policy[i]\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in spGames:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "\n",
    "                while node.is_fully_expanded():\n",
    "                    node = node.select()\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "                value = self.game.get_opponent_value(value)\n",
    "                \n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "                    \n",
    "                else:\n",
    "                    spg.node = node\n",
    "                    \n",
    "            expandable_spGames = [mappingIdx for mappingIdx in range(len(spGames)) if spGames[mappingIdx].node is not None]\n",
    "                    \n",
    "            if len(expandable_spGames) > 0:\n",
    "                states = np.stack([spGames[mappingIdx].node.state for mappingIdx in expandable_spGames])\n",
    "                \n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "                value = value.cpu().numpy()\n",
    "                \n",
    "            for i, mappingIdx in enumerate(expandable_spGames):\n",
    "                node = spGames[mappingIdx].node\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "\n",
    "                node.expand(spg_policy)\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb6930bb-9cac-439c-8150-640d3379b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "        \n",
    "    def selfPlay(self):\n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        spGames = [SPG(self.game) for spg in range(self.args['num_parallel_games'])]\n",
    "        \n",
    "        while len(spGames) > 0:\n",
    "            states = np.stack([spg.state for spg in spGames])\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "            \n",
    "            self.mcts.search(neutral_states, spGames)\n",
    "            \n",
    "            for i in range(len(spGames))[::-1]:\n",
    "                spg = spGames[i]\n",
    "                \n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "\n",
    "                temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n",
    "                action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "\n",
    "                if is_terminal:\n",
    "                    for hist_neutral_state, hist_action_probs, hist_player in spg.memory:\n",
    "                        hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(hist_neutral_state),\n",
    "                            hist_action_probs,\n",
    "                            hist_outcome\n",
    "                        ))\n",
    "                    del spGames[i]\n",
    "                    \n",
    "            player = self.game.get_opponent(player)\n",
    "            \n",
    "        return return_memory\n",
    "                \n",
    "    def train(self, memory):\n",
    "        random.shuffle(memory)\n",
    "        for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] \n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "            \n",
    "            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "            \n",
    "            out_policy, out_value = self.model(state)\n",
    "            \n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def learn(self):\n",
    "        for iteration in range(self.args['num_iterations']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations'] // self.args['num_parallel_games']):\n",
    "                memory += self.selfPlay()\n",
    "                \n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f\"model_{iteration}_{self.game}.pt\")\n",
    "            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}_{self.game}.pt\")\n",
    "            \n",
    "class SPG:\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6862e08c-008b-4008-8b70-992489fdba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iterations': 8,\n",
    "    'num_selfPlay_iterations': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphaZero.learn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20a60f-0c69-44be-841e-86f535fc03c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'dirichlet_epsilon': 0.,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(\"model_7_ConnectFour.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "    \n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid_moves\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "\n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "            \n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "        \n",
    "    state = game.get_next_state(state, action, player)\n",
    "    \n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "    \n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else:\n",
    "            print(\"draw\")\n",
    "        break\n",
    "        \n",
    "    player = game.get_opponent(player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
